{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e832392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570926a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputProjection(nn.Module):\n",
    "    def __init__(self, input_dim=48, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 64, 48)\n",
    "        x = self.linear(x)  # → (batch_size, 64, 256)\n",
    "        x = self.activation(x)\n",
    "        return self.layer_norm(x)  # → (batch_size, 64, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d3fecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim=256, max_len=100):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        nn.init.xavier_uniform_(self.pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 64, 256)\n",
    "        return x + self.pe[:, :x.size(1), :]  # Добавляем позиционную информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc80ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Проекционные матрицы для каждой головы\n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Проецируем в query, key, value\n",
    "        Q = self.w_q(x)  # → (batch_size, 64, 256)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_v(x)\n",
    "        \n",
    "        # Разделяем на головы\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Теперь: (batch_size, num_heads, 64, head_dim)\n",
    "        \n",
    "        # Вычисляем attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Применяем attention к values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Объединяем головы\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Финальная проекция\n",
    "        return self.w_o(attention_output), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "310201af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-attention с residual connection\n",
    "        attn_output, weights = self.attention(x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward с residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3743bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTemporalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=48, embed_dim=64, num_heads=8, \n",
    "                 num_layers=3, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = InputProjection(input_dim, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len=100)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 64, 48)\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_proj(x)  # → (batch_size, 64, 256)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x, weights = block(x)\n",
    "            attention_weights.append(weights)\n",
    "        \n",
    "        return x, torch.stack(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8f4b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TransformerTemporalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8891599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(1, 64, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cb206d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_encoded, attention_weights = enc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2767c485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8123, -0.6182, -0.6583, -0.0629,  2.3469,  1.1953,  0.3435,  0.6787,\n",
       "          1.0185, -0.5536,  0.7050, -1.1617, -0.8173, -0.8221,  0.2815,  0.3436,\n",
       "         -0.0685, -0.4311,  0.2339,  0.7412, -0.1787, -0.1059, -0.0061, -0.4310,\n",
       "         -0.6668, -1.1997, -0.7102, -1.5147,  0.0836, -0.5731, -0.4812, -1.0000,\n",
       "         -0.9866, -0.3735, -0.2992, -1.0261, -1.4122, -0.6642,  1.4058, -0.4298,\n",
       "          1.9099,  1.0879,  0.3377, -0.4276,  2.8044, -1.0083,  2.0285,  0.1321,\n",
       "          0.2960,  0.0583,  1.5483, -0.1285, -1.6143,  0.1691, -0.3700, -1.0194,\n",
       "         -0.9143,  0.7447, -1.3078, -1.4715,  1.5211,  0.0534,  1.8833,  0.7500]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_encoded[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "400fde89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 8, 64, 64])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab4731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab_2.1.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
